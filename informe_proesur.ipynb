{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c135985d",
   "metadata": {},
   "source": [
    "# Informe de desempeño de PROESUR años 2015 - 2024\n",
    "\n",
    "Este informe consta del desempeño del campus PROESUR desde el año 2015 hasta el año 2024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = Path(\"data\")  \n",
    "OUTPUT = Path(\"Extracted_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a136f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_small_chunks(path: Path, col: str = \"Cod_Estable\",code: str = \"05-02-0232-46\") -> pd.DataFrame:\n",
    "    '''\n",
    "    Extracts only the data we want from each xlsx file \n",
    "    ''' \n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    df = df[df[col] == code]\n",
    "    return df\n",
    "\n",
    "def concat_dataframes(files: list[Path]) -> pd.DataFrame | None:\n",
    "    df_list: list[pd.DataFrame] = []\n",
    "    for f in files:\n",
    "        df_list.append(load_small_chunks(f))\n",
    "    try:\n",
    "        result = pd.concat(df_list, ignore_index=True)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919b1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_small_chunks_multiple(path: Path, col: str = \"Cod_Estable\", code: str = \"05-02-0232-46\") -> pd.DataFrame:\n",
    "    \"\"\"Read first sheet, keep ALL columns, filter by the first alias that exists.\"\"\"\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\", dtype=str)  # keep as strings\n",
    "    try: \n",
    "        df = df[df[col] == code]  \n",
    "    except Exception:\n",
    "        df = df[df[col] == code]\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d2c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for f in sorted(FOLDER.glob(\"*.xlsx\"), reverse=True):\n",
    "#     df = load_small_chunks_multiple(f)\n",
    "#     if not df.empty:\n",
    "#         df.to_csv(OUTPUT / f\"{f.stem}_filtered.csv\", index=False)\n",
    "#     else:\n",
    "#         print(f\"{f.stem} was empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f6dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_value(path: Path, val:str, col:str) -> pd.DataFrame:\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    mask = df[col].astype(str).str.contains(val, case=False, na=False)\n",
    "    return df.loc[mask]\n",
    "\n",
    "\n",
    "def search_by_map(path: Path, query: dict[str, str], op: str = \"AND\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    query: {\"NOM_ESTAB\": \"INSTITUTO\", \"Municipio\": \"Guatemala\"}\n",
    "    op: \"AND\" or \"OR\"\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\", dtype=str)\n",
    "    masks = []\n",
    "    for col, val in query.items():\n",
    "        if col in df.columns:\n",
    "            m = df[col].astype(str).str.contains(val, case=False, na=False)\n",
    "            masks.append(m)\n",
    "    if not masks:\n",
    "        return df.iloc[0:0]\n",
    "    if op.upper() == \"OR\":\n",
    "        mask = masks[0]\n",
    "        for m in masks[1:]: mask = mask | m\n",
    "    else:\n",
    "        mask = masks[0]\n",
    "        for m in masks[1:]: mask = mask & m\n",
    "    return df.loc[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27f651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILES = [\"GRADUANDOS 2015.xlsx\", \"GRADUANDOS 2016.xlsx\"]\n",
    "\n",
    "# for fname in FILES:\n",
    "#     file_path = FOLDER / fname\n",
    "#     if not file_path.exists():\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#         continue\n",
    "#     result = search_by_value(file_path, \"INSTITUTO TECNOLOGICO PROESUR \", \"NOM_ESTAB\")\n",
    "#     result.to_csv(OUTPUT / f\"{fname}_filtered.csv\", index=False)\n",
    "#     print(f\"{fname}: {len(result)} rows found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487e225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILES = [\"2019-Grad-Internet.xlsx\", \"2018_-_Grad_Internet.xlsx\", \"2017_Grad-Version_Internet.xlsx\"]\n",
    "# query = {\"Cod_Muni\": \"0502\", \"Direc_Estable\": \"Km. 92.5 Finca Camantulul Carretera A Mazatenango\"}\n",
    "# for fname in FILES:\n",
    "#     file_path = FOLDER / fname\n",
    "#     if not file_path.exists():\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#         continue\n",
    "#     result = search_by_map(file_path, query=query, op=\"AND\")\n",
    "#     result.to_csv(OUTPUT / f\"{fname}_filtered.csv\", index=False)\n",
    "#     print(f\"{fname}: {len(result)} rows found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ad4ff",
   "metadata": {},
   "source": [
    "## Análisis de los datos obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALIASES = {\n",
    "\n",
    "    # academic \n",
    "    \"nivel_mate\": [\n",
    "        \"DESEMPEÑO_MATEMÁTICAS\", \"Desempeño_Mate\"\n",
    "    ],\n",
    "    \"nivel_lectura\": [\n",
    "        \"DESEMPEÑO_LECTURA\", \"Desempeño_Lect\"\n",
    "    ],  \n",
    "    \"logro_mate\": [\n",
    "        \"LOGRO_MATEMÁTICAS\", \"Logro_Mate\"\n",
    "    ],\n",
    "    \"logro_lectura\": [\n",
    "        \"LOGRO_LECTURA\", \"Logro_Lect\"\n",
    "    ],\n",
    "    \"periodos_semanales_mate\" : [\n",
    "        \"MAT_PERIODOS_MATEMATICAS_SEMANA_Recodificada\", \"Mate_Periodos_Matematicas_Semana_RECO\",\"Mate_Periodos_Matematicas_Semana_Reco\"\n",
    "    ],\n",
    "    \"periodos_semanales_lectura\" : [\n",
    "        \"LEC_PERIODOS_LECTURA_SEMANA_Recodificada\", \"Lect_Periodos_Lectura_Semana_RECO\", \"Lect_Periodos_Lectura_Semana_Reco\"\n",
    "    ],\n",
    "    # socioeconomic \n",
    "    \"trabaja\": [\n",
    "       \"ED_TRABAJA_ACTUALMENTE\", \"Ed_Trabaja_Actualmente\"\n",
    "    ],\n",
    "    \"acceso_internet\": [\n",
    "        \"CC_SERVICIO_INTERNET\", \"CC_Servicio_Internet\", \"Sc_Servicio_Internet\"\n",
    "    ],\n",
    "    \"asistencia_primaria\" : [\n",
    "        \"ED_ASISTIO_PREPRIMARIA\" , \"Ed_Asistio_Preprimaria\"\n",
    "    ],\n",
    "    \"sexo\": [\n",
    "        \"GENERO\", \"Sexo_RECO\", \"Sexo_Reco\"\n",
    "    ],\n",
    "    \"padre_asistio_escuela\" : [\n",
    "        \"FM_ASISTIO_ESCUELA_PAPA\", \"Fm_Asistio_Escuela_Papa\"\n",
    "    ],\n",
    "    \"grado_alcanzado_padre\" : [\n",
    "        \"FM_GRADO_ALCANZO_PAPA_Recodificada\", \"Fm_Grado_Alcanzo_Papa_RECO\"\n",
    "    ],\n",
    "    \"madre_asistio_escuela\" : [\n",
    "        \"FM_ASISTIO_ESCUELA_MAMA\", \"Fm_Asistio_Escuela_Mama\"\n",
    "    ],\n",
    "    \"grado_alcanzado_madre\" : [\n",
    "        \"FM_GRADO_ALCANZO_MAMA_Recodificada\", \"Fm_Grado_Alcanzo_Mama_RECO\"\n",
    "    ],\n",
    "    \"grupo_etnico\" : [\n",
    "        \"IE_IDENTIFICACION_ETNICA_Recodificada\" , \"Identificacion_Etnica_RECO\"\n",
    "    ],\n",
    "    \"rep_primaria\" : [\n",
    "        \"ED_REPITIO_ALGUN_GRADO_PRIMARIA\", \"Ed_Repitio_Algun_Grado_Primaria\", \"Ed_Repitio_Algun_Grado\"\n",
    "    ]\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6270d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col(df: pd.DataFrame, key: str) -> str | None:\n",
    "    \"\"\"Return the actual column name for a logical key, or None if not present.\"\"\"\n",
    "    for name in ALIASES.get(key, []):\n",
    "        if name in df.columns:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def dist_levels(df: pd.DataFrame, key: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns percentage distribution of categories in the column mapped by `key`.\n",
    "    Example: key=\"nivel_mate\" or \"nivel_lectura\".\n",
    "    \"\"\"\n",
    "    col = get_col(df, key)\n",
    "    if not col:\n",
    "        print(\"No column found\")\n",
    "        return pd.Series(dtype=float)\n",
    "    \n",
    "    s = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "        .replace({\"S/ DATO\": None, \"NA\": None, \"NAN\": None})\n",
    "    )\n",
    "    return (s.value_counts(dropna=True, normalize=True) * 100).round(2)\n",
    "\n",
    "\n",
    "# TODO modify to use dist_levels to map percentages\n",
    "def pct_true(df: pd.DataFrame, key: str) -> float | None:\n",
    "    \"\"\"\n",
    "    Percentage of 'true' / 'yes' / 1 for the column mapped by `key`.\n",
    "    Returns None if column not present or no valid data.\n",
    "    \"\"\"\n",
    "    col = get_col(df, key)\n",
    "    if not col:\n",
    "        print(\"Column was None\")\n",
    "        return None\n",
    "    \n",
    "    s = df[col].astype(str).str.strip().str.lower()\n",
    "    true_vals = {\"1\"}\n",
    "    false_vals = {\"0\"}\n",
    "    \n",
    "    mask_valid = s.isin(true_vals | false_vals)\n",
    "    if not mask_valid.any():\n",
    "        print(\"Not found any in mask\")\n",
    "        return None\n",
    "    \n",
    "    pct = (s.isin(true_vals) & mask_valid).mean() * 100\n",
    "    return round(pct, 2)\n",
    "\n",
    "def count_values(df: pd.DataFrame, column: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Counts each unique value in the specified column.\n",
    "    Returns a Series sorted by count (descending).\n",
    "    \"\"\"\n",
    "    col = get_col(df, column)\n",
    "    if not col:\n",
    "        return pd.Series(dtype=int)\n",
    "\n",
    "\n",
    "    return df[col].value_counts(dropna=False)\n",
    "\n",
    "\n",
    "def val_distribution(df: pd.DataFrame, key: str) -> pd.DataFrame:\n",
    "\n",
    "    col = get_col(df, key)\n",
    "    if not col:\n",
    "        return pd.DataFrame()\n",
    "    counts = df[col].value_counts(dropna=False)\n",
    "    percents = (counts / counts.sum() * 100).round(2)\n",
    "    return pd.DataFrame({\"Count\" : counts, \"Percent\" : percents}).sort_values(\"Count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def value_dist(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns value/count/percent for col.\n",
    "    Only non-empty distributions.\n",
    "    \"\"\"\n",
    "    counts = df[col].value_counts(dropna=False)\n",
    "    if counts.empty or counts.sum() == 0:\n",
    "        return pd.DataFrame(columns=[\"value\", \"count\", \"percent\"])\n",
    "\n",
    "    perc = (counts / counts.sum() * 100).round(2)\n",
    "\n",
    "    out = (\n",
    "        pd.DataFrame({\n",
    "            \"value\": counts.index.astype(str),\n",
    "            \"count\": counts.values,\n",
    "            \"percent\": perc.values,\n",
    "        })\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def analyze_file_to_metrics(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For a single CSV, compute distributions for all ALIASES keys.\n",
    "\n",
    "    Returns a DataFrame with:\n",
    "        variable, category, count, percent\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for key in ALIASES.keys():\n",
    "        col = get_col(df, key)\n",
    "        if not col:\n",
    "            continue\n",
    "\n",
    "        dist = value_dist(df, col)\n",
    "        if dist.empty:\n",
    "            continue\n",
    "\n",
    "        for _, r in dist.iterrows():\n",
    "            val = str(r[\"value\"]).strip()\n",
    "            # skip NaN labels if they appear as 'nan'\n",
    "            if val.lower() == \"nan\":\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"variable\": key,\n",
    "                \"category\": val,\n",
    "                \"count\": int(r[\"count\"]),\n",
    "                \"percent\": float(r[\"percent\"]),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91049aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(\n",
    "    folder: Path,\n",
    "    filenames: list[str] | None = None,   # pass to test specific files\n",
    "    pattern: str = \"*.csv\"                # used if filenames is None\n",
    "):\n",
    "    out_dir = folder / \"metrics\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # decide which CSVs to process\n",
    "    if filenames is not None:\n",
    "        csv_paths = []\n",
    "        for name in filenames:\n",
    "            p = folder / name\n",
    "            if p.exists():\n",
    "                csv_paths.append(p)\n",
    "            else:\n",
    "                print(f\"⚠️ Missing file (skipped): {p}\")\n",
    "    else:\n",
    "        csv_paths = sorted(folder.glob(pattern))\n",
    "\n",
    "    for csv_path in csv_paths:\n",
    "        metrics_df = analyze_file_to_metrics(csv_path)\n",
    "\n",
    "        if metrics_df.empty:\n",
    "            print(f\"{csv_path.name}: no metrics found (check ALIASES or data).\")\n",
    "            continue\n",
    "\n",
    "        out_path = out_dir / f\"{csv_path.stem}__metrics.csv\"\n",
    "        metrics_df.to_csv(out_path, index=False)\n",
    "        print(f\"{csv_path.name}: wrote {out_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b09e2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_PATH = Path(\"Extracted_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb108924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADUANDOS 2015.xlsx_filtered.csv: wrote GRADUANDOS 2015.xlsx_filtered__metrics.csv\n",
      "GRADUANDOS 2016.xlsx_filtered.csv: wrote GRADUANDOS 2016.xlsx_filtered__metrics.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_reports: list[str] = [\"GRADUANDOS 2015.xlsx_filtered.csv\", \"GRADUANDOS 2016.xlsx_filtered.csv\"]\n",
    "\n",
    "run_all(CLEAN_PATH, early_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e629c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017_Grad-Version_Internet.xlsx_filtered.csv: wrote 2017_Grad-Version_Internet.xlsx_filtered__metrics.csv\n",
      "2018_-_Grad_Internet.xlsx_filtered.csv: wrote 2018_-_Grad_Internet.xlsx_filtered__metrics.csv\n",
      "2019-Grad-Internet.xlsx_filtered.csv: wrote 2019-Grad-Internet.xlsx_filtered__metrics.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_reports: list[str] = [\"2017_Grad-Version_Internet.xlsx_filtered.csv\", \"2018_-_Grad_Internet.xlsx_filtered.csv\", \"2019-Grad-Internet.xlsx_filtered.csv\"]\n",
    "\n",
    "run_all(CLEAN_PATH, early_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281b4b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-Grad-Internet_filtered.csv: wrote 2022-Grad-Internet_filtered__metrics.csv\n",
      "2023-Grad-Internet_filtered.csv: wrote 2023-Grad-Internet_filtered__metrics.csv\n",
      "2024-Grad-Internet_filtered.csv: wrote 2024-Grad-Internet_filtered__metrics.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_reports: list[str] = [\"2022-Grad-Internet_filtered.csv\", \"2023-Grad-Internet_filtered.csv\", \"2024-Grad-Internet_filtered.csv\"]\n",
    "\n",
    "run_all(CLEAN_PATH, early_reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
